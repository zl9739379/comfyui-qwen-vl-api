import requests
from PIL import Image
from io import BytesIO
import numpy as np
import torch
import base64
import json

class VL_QwenDescribeImage:
    def __init__(self):
        pass
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "image": ("IMAGE",),
            },
            "optional": {
                "api_url": ("STRING", {
                    "default": "https://api.siliconflow.cn/v1",
                    "multiline": False,
                    "tooltip": "APIæœåŠ¡å™¨åœ°å€"
                }),
                "model": ("STRING", {
                    "default": "Qwen/Qwen2.5-VL-72B-Instruct",
                    "multiline": False,
                    "tooltip": "ä½¿ç”¨çš„è§†è§‰è¯­è¨€æ¨¡å‹"
                }),
                "prompt": ("STRING", {
                    "default": "è¯·æè¿°è¿™å¼ å›¾ç‰‡çš„å†…å®¹ã€‚",
                    "multiline": True,
                    "tooltip": "ç»™æ¨¡å‹çš„æç¤ºè¯ï¼Œæè¿°ä½ å¸Œæœ›æ¨¡å‹å¦‚ä½•åˆ†æå›¾ç‰‡"
                }),
                "api_key": ("STRING", {
                    "default": "",
                    "multiline": False,
                    "tooltip": "APIå¯†é’¥ï¼Œå¦‚æœéœ€è¦è®¤è¯è¯·å¡«å…¥"
                }),
                "timeout": ("INT", {
                    "default": 60,
                    "min": 10,
                    "max": 300,
                    "step": 10,
                    "tooltip": "è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰"
                }),
                "max_tokens": ("INT", {
                    "default": 1000,
                    "min": 100,
                    "max": 4000,
                    "step": 100,
                    "tooltip": "æœ€å¤§è¿”å›tokenæ•°é‡"
                }),
                "temperature": ("FLOAT", {
                    "default": 0.7,
                    "min": 0.0,
                    "max": 2.0,
                    "step": 0.1,
                    "tooltip": "ç”Ÿæˆæ–‡æœ¬çš„éšæœºæ€§ï¼Œ0ä¸ºæœ€ç¡®å®šï¼Œ2ä¸ºæœ€éšæœº"
                }),
                "image_quality": ("INT", {
                    "default": 95,
                    "min": 50,
                    "max": 100,
                    "step": 5,
                    "tooltip": "ä¸Šä¼ å›¾ç‰‡çš„JPEGè´¨é‡"
                }),
                "detail": (["auto", "low", "high"], {
                    "default": "auto",
                    "tooltip": "å›¾åƒå¤„ç†è¯¦ç»†ç¨‹åº¦ï¼šauto=è‡ªåŠ¨é€‰æ‹©ï¼Œlow=ä½åˆ†è¾¨ç‡å¿«é€Ÿå¤„ç†ï¼Œhigh=é«˜åˆ†è¾¨ç‡è¯¦ç»†åˆ†æ"
                }),
            }
        }
    
    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("description",)
    FUNCTION = "describe"
    CATEGORY = "VL Model"
    
    def describe(self, image, api_url="https://api.siliconflow.cn/v1", model="Qwen/Qwen2.5-VL-72B-Instruct", 
                 prompt="è¯·æè¿°è¿™å¼ å›¾ç‰‡çš„å†…å®¹ã€‚", api_key="", timeout=60, max_tokens=1000, 
                 temperature=0.7, image_quality=95, detail="auto"):
        try:
            # å¤„ç†ComfyUIçš„å›¾åƒæ ¼å¼ (batch, height, width, channels)
            if isinstance(image, torch.Tensor):
                img_array = image[0].cpu().numpy()
                img_array = (img_array * 255).astype(np.uint8)
            else:
                img_array = image[0] if len(image.shape) == 4 else image
                img_array = (img_array * 255).astype(np.uint8) if img_array.max() <= 1.0 else img_array.astype(np.uint8)
            
            # è½¬æ¢ä¸ºPIL Image
            img = Image.fromarray(img_array)
            
            # å°†å›¾ç‰‡ç¼–ç ä¸ºbase64
            buffered = BytesIO()
            img.save(buffered, format="JPEG", quality=image_quality)
            img_base64 = base64.b64encode(buffered.getvalue()).decode('utf-8')
            
            # å‡†å¤‡OpenAIé£æ ¼çš„è¯·æ±‚å¤´
            headers = {
                "Content-Type": "application/json",
                "Accept": "application/json"
            }
            if api_key and api_key.strip():
                headers["Authorization"] = f"Bearer {api_key.strip()}"
            
            # å‡†å¤‡OpenAIé£æ ¼çš„è¯·æ±‚ä½“
            payload = {
                "model": model,
                "messages": [
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "text",
                                "text": prompt
                            },
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/jpeg;base64,{img_base64}",
                                    "detail": detail
                                }
                            }
                        ]
                    }
                ],
                "max_tokens": max_tokens,
                "temperature": temperature
            }
            
            # å‘é€è¯·æ±‚åˆ° /chat/completions ç«¯ç‚¹
            response = requests.post(
                f"{api_url.rstrip('/')}/chat/completions",
                headers=headers,
                json=payload,
                timeout=timeout
            )
            response.raise_for_status()
            result = response.json()
            
            # è§£æOpenAIé£æ ¼çš„å“åº”
            if "choices" in result and len(result["choices"]) > 0:
                message = result["choices"][0].get("message", {})
                description = message.get("content", "æ— è¿”å›ç»“æœ")
            else:
                description = result.get("content", result.get("text", "æ— è¿”å›ç»“æœ"))
            
            return (description,)
            
        except requests.exceptions.RequestException as e:
            error_msg = f"[ç½‘ç»œé”™è¯¯] {str(e)}"
            if hasattr(e, 'response') and e.response is not None:
                try:
                    error_detail = e.response.json()
                    error_msg += f"\nè¯¦ç»†ä¿¡æ¯: {error_detail}"
                except:
                    error_msg += f"\nçŠ¶æ€ç : {e.response.status_code}"
            return (error_msg,)
        except Exception as e:
            return (f"[é”™è¯¯] {str(e)}",)

# æ³¨å†ŒèŠ‚ç‚¹
NODE_CLASS_MAPPINGS = {
    "VL_QwenDescribeImage": VL_QwenDescribeImage
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "VL_QwenDescribeImage": "ğŸ“· Qwen VL Describe Image (OpenAI API)"
}
